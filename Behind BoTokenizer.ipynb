{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoTokenizer is nothing more than a convenience class that makes use of several tools in pybo to constitute a tokenizer.\n",
    "\n",
    "If many of the available tools are used in the tokenizer, it is because pybo's initial aim was to answer the need of a tokenizer for Tibetan. When other needs will arise, other tools and modules will be added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing means correctly identifying words within an input string.\n",
    "\n",
    "If Tibetan was a language that had clear and unambiguous word boundaries, we might have opted for a negative strategy: identifying the words by identifying what is between words. Then only find an appropriate way to deal with the exceptions.\n",
    "\n",
    "However, Tibetan only seperates syllables with tseks/dots, so we need to adopt an opposite strategy: for a given starting point in the input string, find out how many words can fit in the following syllables, decide which one from all the candidates is correct and then decide the next word will start from that point onwards.\n",
    "\n",
    "Given the words `ab`, `abc`, `ba` and `cde` and the input string `abcdefgh`, here is how we would proceed:\n",
    "\n",
    "1. Starting point: 0.\n",
    "2. Check how many words can fit in from that point: `ab` and `abc` are found\n",
    "3. Decide what word is the correct one in the present context: `ab` (arbitrary decision in this example)\n",
    "4. Starting point: 2.\n",
    "5. Check how many words can fit in from that point: `cde` is found\n",
    "6. Decide what word is the correct one: `cde` (the only one)\n",
    "7. Starting point: 5.\n",
    "8. Check how many words can fit in from that point: none.\n",
    "9. `f` is decided to be a non-word token.\n",
    "10. Starting point: 6.\n",
    "11. Check how many words can fit in from that point: none.\n",
    "12. `g` is decided to be a non-word token.\n",
    "13. Starting point: 7.\n",
    "14. Check how many words can fit in from that point: none.\n",
    "15. `h` is decided to be a non-word token.\n",
    "\n",
    "In the end, the chain of tokens is: [`ab`, `cde`, `f`, `g`, `h`]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building-blocks of a tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we see the tokenizer will resolve around two things:\n",
    "\n",
    " 1. lexical resources: a list of all the valid Tibetan words\n",
    " 2. a mechanism to walk the input string while deciding where tokens start and end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lexical resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw lists of words are stored [here](./pybo/resources/trie) inside the `resources` folder of pybo.\n",
    "\n",
    "These lists need to be crafted with great care. The produced tokens directly depend on the content of these lists. Let's say we had an additional word `fg` in the example above, or that the word `ab` was missing. The output would be completely different.\n",
    "\n",
    "The difficulty in knowing what word to add or to remove in our lists is to define the limit between a regular word and a compounded word. We don't want to include compounded expressions, compounded words or concatenated words in our lists, except for a few exceptions. Yet we find a lot of those in Tibetan dictionaries. Some even list full titles as dictionary entries.\n",
    "\n",
    "Because of this situation, we provide means to add or deactivate entries in the trie structure used to host the lexical entries in the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  pybotrie.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyBoTrie builds on BasicTrie (that is subclasses) to provide higher-level facilities that are used for tokenizing purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building and saving the trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Trie...\n",
      "Time: 2.3894572257995605\n"
     ]
    }
   ],
   "source": [
    "from pybo import PyBoTrie, BoSyl\n",
    "\n",
    "trie = PyBoTrie(BoSyl(), profile='POS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the message printed, it only loaded the trie that was pickled and saved on disk. PyBoTrie checks for the existence of a trie on disk before deciding to build one from the lexical resources. \n",
    "\n",
    "You can also choose to rebuild a trie like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building Trie... Time: 6.829641819000244\n"
     ]
    }
   ],
   "source": [
    "trie.rebuild_trie()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This functionality comes in handy when you modify an existing trie by either deactivating a word or to add entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exists': True, 'data': 'NOUNᛃᛃᛃ'}\n"
     ]
    }
   ],
   "source": [
    "word = 'བཀྲ་ཤིས་'\n",
    "print(trie.has_word(word))  # inherited from BasicTrie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the trie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`has_word()` loops over every character in its argument and walks down the trie. If it can't go until the end of the input string, the word is not present in the lexical resources. If it reaches the end, it checks that it reached the end of a word (to avoid matching half-words) and returns the information stored with that word.\n",
    "\n",
    "Here is what is actually happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial value of current_node: ['ཀ', 'ག', 'ད', 'བ', 'མ', 'འ', 'ས', 'ཡ', 'ཐ', 'ར', 'ཤ', 'ན', 'པ', 'ཅ', 'ཌ', 'ཧ', 'ཏ', 'ལ', 'ཨ', 'ཛ', 'ཙ', 'ཝ', 'ཁ', 'ང', 'ཆ', 'ཇ', 'ཉ', 'ཕ', 'ཚ', 'ཞ', 'ཟ', 'ཊ', 'ཥ', '༺', '༐', 'ཪ', 'ྐ', '྄', 'ཋ', 'ཎ', ' ', 'ྨ', 'ྴ']\n",
      "\n",
      "0: \"བ\"\tletter is in current_node: True\n",
      "\tnew value: ['ཅ', 'ཙ', 'ར', 'ཱ', 'ི', 'ུ', 'ཻ', 'ཛ', 'ེ', 'ྷ', 'ན', '་', 'ཀ', 'ས', 'ག', 'ཏ', 'ལ', 'ད', 'འ', 'ང', 'བ', 'མ', 'ོ', 'ྱ', 'ྲ', 'ླ', 'ཞ', 'ཟ', 'ཤ', 'ཉ', 'ཊ', 'ཎ', 'པ', 'ཕ', 'ཡ', 'ཥ', 'ཧ', 'ཽ', 'ཾ', 'ྠ', 'ྨ', 'ྀ']\n",
      "\n",
      "1: \"ཀ\"\tletter is in current_node: True\n",
      "\tnew value: ['ག', 'ང', 'ར', 'ས', 'འ', 'ུ', 'ོ', 'ྲ', 'ད', 'ན', 'བ', 'ལ', 'ྱ', 'ླ', 'ྟ', 'ྐ', 'ྚ', 'ྭ', 'ྵ']\n",
      "\n",
      "2: \"ྲ\"\tletter is in current_node: True\n",
      "\tnew value: ['་', 'མ', 'ེ', 'ག', 'བ', 'ལ', 'ས', 'ི', 'ུ', 'ོ', 'ྀ', 'ཱ', 'ཾ', 'ཿ']\n",
      "\n",
      "3: \"་\"\tletter is in current_node: True\n",
      "\tnew value: ['ཤ', 'བ', 'ཝ', 'ར', 'མ']\n",
      "\n",
      "4: \"ཤ\"\tletter is in current_node: True\n",
      "\tnew value: ['ི', 'ྲ']\n",
      "\n",
      "5: \"ི\"\tletter is in current_node: True\n",
      "\tnew value: ['ས']\n",
      "\n",
      "6: \"ས\"\tletter is in current_node: True\n",
      "\tnew value: ['་']\n",
      "\n",
      "7: \"་\"\tletter is in current_node: True\n",
      "\tnew value: ['ག', 'ད', 'པ', 'མ', 'ཚ', 'ཤ']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setting the current node to the root of the trie\n",
    "current_node = trie.head\n",
    "print(f'initial value of current_node: {list(current_node.children.keys())}\\n')\n",
    "for n, letter in enumerate(word):\n",
    "    print(f'{n}: \"{letter}\"', end='\\t')\n",
    "    if current_node:  # ensures we can continue walking\n",
    "        print(f'letter is in current_node: {letter in current_node.children}')\n",
    "        current_node = trie.walk(letter, current_node)\n",
    "        \n",
    "        print(f'\\tnew value: {list(current_node.children.keys())}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fed the whole word to the trie and that we see current_node is still not `None`, we have to check that we are not in the middle of a word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word exists as an entry in the lexical resources: True\n",
      "the data about this word stored in the trie is: \"NOUNᛃᛃᛃ\"\n"
     ]
    }
   ],
   "source": [
    "print(f'word exists as an entry in the lexical resources: {current_node.is_match()}')\n",
    "print(f'the data about this word stored in the trie is: \"{current_node.data}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it short, in order to use PyBoTrie, we need to:\n",
    " - store the current node in a variable (`trie.head` the first time)\n",
    " - use `walk()` to go one step down the trie\n",
    " - use `is_match()` to know if we have a match or not\n",
    " - use the content of the other attributes of the node (`data`, `freq` and `skrt`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modifying the trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exists': False}\n",
      "{'exists': True, 'data': 'NOUNᛃᛃᛃ'}\n"
     ]
    }
   ],
   "source": [
    "trie.remove_word(word)  # inherited from BasicTrie\n",
    "print(trie.has_word(word))\n",
    "\n",
    "trie.inflect_n_add(word, ins='data', pos='NOUN')\n",
    "print(trie.has_word(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`inflect_n_add()` adds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can check for the existence of a given word in our trie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exists': True, 'data': 'NOUNᛃᛃᛃ'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trie.has_word('བཀྲ་ཤིས་')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to see if we have a couple of other words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exists': True, 'data': 'NOUNᛃᛃᛃ'}\n",
      "{'exists': True, 'data': 'NOUNᛃlaᛃ1ᛃaa'}\n",
      "{'exists': True, 'data': 'NOUNᛃgi+oᛃ4ᛃaa'}\n"
     ]
    }
   ],
   "source": [
    "worda = 'མཐའ་'\n",
    "worda_affixed1 = 'མཐར་'\n",
    "worda_affixed2 = 'མཐའིའོ་'\n",
    "for word in [worda, worda_affixed1, worda_affixed2]:\n",
    "    print(trie.has_word(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three words exist, yet present differing data:\n",
    "\n",
    "The first one doesn't have any information about affixed particles.\n",
    "\n",
    "The second one says that:\n",
    " - the word's Part-Of-Speech is a noun\n",
    " - the affixed particle is a ladon(ལ་དོན།)\n",
    " - the affixed particle spans 1 character(ར)\n",
    " - the hosting word ends with a འ in its unaffixed form (མཐའ་).\n",
    "\n",
    "The third one shows that \n",
    " - its POS is also a noun\n",
    " - the affixed particles are:\n",
    "     - a genetive (བྱེད་སྒྲ་): gi(གི་) is the chosen canonical form of this case\n",
    "     - a terminative (རྫོགས་ཚིག་): o(འོ་) is the chosen form.\n",
    " - the affixed particle spans 4 characters (འིའོ)\n",
    " - the hosting word ends with a འ in its unaffixed form (མཐའ་)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exists': True, 'data': 'NOUNᛃᛃᛃ'}\n",
      "{'exists': True, 'data': 'OTHERᛃᛃᛃ'}\n",
      "{'exists': True, 'data': 'NOUNᛃangᛃ2ᛃ'}\n"
     ]
    }
   ],
   "source": [
    "wordb = 'རྒྱ་མཚོ་'\n",
    "wordb_affixed1 = 'རྒྱ་མཚོའམ་'\n",
    "wordb_affixed2 = 'རྒྱ་མཚོའང་'\n",
    "for word in [wordb, wordb_affixed1, wordb_affixed2]:\n",
    "    print(trie.has_word(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
