{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing an input string is the basis needed to do higher-level operations such as tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = '༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pybotextchunks.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is a wrapper around PyBoChunk (that it subclasses).\n",
    "Its main purpose is to provide the input to be fed into the trie within tokenizer.py\n",
    "\n",
    "Here is what it produces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output is a <class 'list'> of <class 'tuple'> containing each 2 elements.\n",
      "\tex:  (None, (102, 0, 2))\n",
      "The first element is either a <class 'NoneType'> or a <class 'list'> of ints\n",
      "\tex: None or [2, 3]\n"
     ]
    }
   ],
   "source": [
    "from pybo import PyBoTextChunks\n",
    "text_chunks = PyBoTextChunks(input_str)\n",
    "\n",
    "output = text_chunks.serve_syls_to_trie()\n",
    "print(f'The output is a {type(output)} of {type(output[0])} containing each {len(output[0])} elements.')\n",
    "print('\\tex: ', output[0])\n",
    "print(f'The first element is either a {type(output[0][0])} or a {type(output[1][0])} of ints')\n",
    "print('\\tex:', output[0][0], 'or', output[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See below for the second element, it is the output of PyBoChunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\t None\n",
      "2.\t [2, 3]\n",
      "3.\t [5, 6, 7]\n",
      "4.\t [9, 10, 11]\n",
      "5.\t None\n",
      "6.\t [18, 19, 20]\n",
      "7.\t None\n",
      "8.\t [23, 24, 26, 27]\n",
      "9.\t None\n",
      "10.\t [30, 31, 32]\n",
      "11.\t [34, 35, 36]\n",
      "12.\t [38, 39, 40]\n",
      "13.\t [42, 43, 44, 45]\n",
      "14.\t None\n",
      "15.\t [50, 51]\n",
      "16.\t None\n",
      "17.\t [54, 55, 56, 57]\n",
      "18.\t [59, 60, 61]\n",
      "19.\t [63, 64, 65, 66]\n",
      "20.\t [68, 69, 70]\n",
      "21.\t [72, 73, 74]\n",
      "22.\t [76, 77]\n",
      "23.\t [79, 80]\n",
      "24.\t [82, 83, 84, 85]\n",
      "25.\t None\n"
     ]
    }
   ],
   "source": [
    "for n, trie_input in enumerate(output):\n",
    "    print(f'{n+1}.\\t {trie_input[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first element provides the information to the trie that a given chunk is a syllable, and thus should be fed to the trie, or that it is not a syllable.\n",
    "\n",
    "In case it is a syllable, its elements are indices of the characters that constitute the syllable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།\n",
      "1.\n",
      "2.\t['ཤ', 'ི', '་']\n",
      "3.\t['བ', 'ཀ', 'ྲ', '་']\n",
      "4.\t['ཤ', 'ི', 'ས', '་']\n",
      "5.\n",
      "6.\t['བ', 'ད', 'ེ', '་']\n",
      "7.\n",
      "8.\t['ལ', 'ེ', 'ག', 'ས', '་']\n",
      "9.\n",
      "10.\t['བ', 'ཀ', 'ྲ', '་']\n",
      "11.\t['ཤ', 'ི', 'ས', '་']\n",
      "12.\t['བ', 'ད', 'ེ', '་']\n",
      "13.\t['ལ', 'ེ', 'ག', 'ས', '་']\n",
      "14.\n",
      "15.\t['ཀ', 'ཀ', '་']\n",
      "16.\n",
      "17.\t['མ', 'ཐ', 'འ', 'ི', '་']\n",
      "18.\t['ར', 'ྒ', 'ྱ', '་']\n",
      "19.\t['མ', 'ཚ', 'ོ', 'ར', '་']\n",
      "20.\t['ག', 'ན', 'ས', '་']\n",
      "21.\t['པ', 'འ', 'ི', '་']\n",
      "22.\t['ཉ', 'ས', '་']\n",
      "23.\t['ཆ', 'ུ', '་']\n",
      "24.\t['འ', 'ཐ', 'ུ', 'ང', '་']\n",
      "25.\n"
     ]
    }
   ],
   "source": [
    "print('\\t', input_str)\n",
    "for n, trie_input in enumerate(output):\n",
    "    syl = trie_input[0]\n",
    "    if syl:\n",
    "        syllable = [input_str[s] for s in syl] + ['་']\n",
    "        print(f'{n+1}.\\t{syllable}')\n",
    "    else:\n",
    "        print(f'{n+1}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation chunks are left aside, non-Tibetan parts also and the content of the syllable has been cleaned.\n",
    "In token 6, the double tsek has been normalized to a single tsek.\n",
    "In token 7, the space in the middle has been left aside.\n",
    "On top of this, the non-syllabic chunks give us a cue that any ongoing word has to end there.\n",
    "\n",
    "Now, this cleaned content can be fed in the trie, syllable after syllable, character after character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pybochunk.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is powered by the chunking framework defined in BoChunk (that it subclasses).\n",
    "\n",
    "It implements the following chunking pipeline:\n",
    " - turn the input string into a sequence of alternating \"bo\" and \"non-bo\" chunks\n",
    " - turn the \"bo\" chunks into a sequence of alternating \"punct\" and \"bo\" chunks\n",
    " - turn the remaining \"bo\" chunks into a sequence of alternating \"sym\" and \"bo\" chunks\n",
    " - turn the remaining \"bo\" chunks into a sequence of alternating \"num\" and \"bo\" chunks\n",
    " - turn the remaining \"bo\" chunks into syllables\n",
    " - concatenate the chunks containing only spaces to the preceding one\n",
    "\n",
    "The last pipe is created by using the building blocks of BoChunk, but implemented here, since this treatment merges two chunks given a condition, while BoChunk is a chunking framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybo import PyBoChunk\n",
    "\n",
    "chunks = PyBoChunk(input_str)\n",
    "output = chunks.chunk()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\t(102, 0, 2)\n",
      "2.\t(106, 2, 3)\n",
      "3.\t(106, 5, 4)\n",
      "4.\t(106, 9, 6)\n",
      "5.\t(101, 15, 3)\n",
      "6.\t(106, 18, 4)\n",
      "7.\t(102, 22, 1)\n",
      "8.\t(106, 23, 5)\n",
      "9.\t(102, 28, 2)\n",
      "10.\t(106, 30, 4)\n",
      "11.\t(106, 34, 4)\n",
      "12.\t(106, 38, 4)\n",
      "13.\t(106, 42, 5)\n",
      "14.\t(109, 47, 3)\n",
      "15.\t(106, 50, 2)\n",
      "16.\t(102, 52, 2)\n",
      "17.\t(106, 54, 5)\n",
      "18.\t(106, 59, 4)\n",
      "19.\t(106, 63, 5)\n",
      "20.\t(106, 68, 4)\n",
      "21.\t(106, 72, 4)\n",
      "22.\t(106, 76, 3)\n",
      "23.\t(106, 79, 3)\n",
      "24.\t(106, 82, 5)\n",
      "25.\t(102, 87, 5)\n"
     ]
    }
   ],
   "source": [
    "for n, chunk in enumerate(output):\n",
    "    print(f'{n+1}.\\t{chunk}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last two elements in the tuple are:\n",
    " - the starting index of the current chunk in the input string\n",
    " - the length of the current chunk\n",
    " \n",
    "PyBoChunk provides a method to replace these two indices with the actual substring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\t(102, '༆ ')\n",
      "2.\t(106, 'ཤི་')\n",
      "3.\t(106, 'བཀྲ་')\n",
      "4.\t(106, 'ཤིས་  ')\n",
      "5.\t(101, 'tr ')\n",
      "6.\t(106, 'བདེ་')\n",
      "7.\t(102, '་')\n",
      "8.\t(106, 'ལེ གས')\n",
      "9.\t(102, '། ')\n",
      "10.\t(106, 'བཀྲ་')\n",
      "11.\t(106, 'ཤིས་')\n",
      "12.\t(106, 'བདེ་')\n",
      "13.\t(106, 'ལེགས་')\n",
      "14.\t(109, '༡༢༣')\n",
      "15.\t(106, 'ཀཀ')\n",
      "16.\t(102, '། ')\n",
      "17.\t(106, 'མཐའི་')\n",
      "18.\t(106, 'རྒྱ་')\n",
      "19.\t(106, 'མཚོར་')\n",
      "20.\t(106, 'གནས་')\n",
      "21.\t(106, 'པའི་')\n",
      "22.\t(106, 'ཉས་')\n",
      "23.\t(106, 'ཆུ་')\n",
      "24.\t(106, 'འཐུང་')\n",
      "25.\t(102, '།། །།')\n"
     ]
    }
   ],
   "source": [
    "with_substrings = chunks.get_chunked(output)\n",
    "for n, chunk in enumerate(with_substrings):\n",
    "    print(f'{n+1}.\\t{chunk}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could do it manually, though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"༆ \" equals \"༆ \"\n"
     ]
    }
   ],
   "source": [
    "chunk_str = input_str[output[0][1]:output[0][1]+output[0][2]]\n",
    "print(f'\"{with_substrings[0][1]}\" equals \"{chunk_str}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get human-readable values for the first int:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\t('punct', '༆ ')\n",
      "2.\t('syl', 'ཤི་')\n",
      "3.\t('syl', 'བཀྲ་')\n",
      "4.\t('syl', 'ཤིས་  ')\n",
      "5.\t('non-bo', 'tr ')\n",
      "6.\t('syl', 'བདེ་')\n",
      "7.\t('punct', '་')\n",
      "8.\t('syl', 'ལེ གས')\n",
      "9.\t('punct', '། ')\n",
      "10.\t('syl', 'བཀྲ་')\n",
      "11.\t('syl', 'ཤིས་')\n",
      "12.\t('syl', 'བདེ་')\n",
      "13.\t('syl', 'ལེགས་')\n",
      "14.\t('num', '༡༢༣')\n",
      "15.\t('syl', 'ཀཀ')\n",
      "16.\t('punct', '། ')\n",
      "17.\t('syl', 'མཐའི་')\n",
      "18.\t('syl', 'རྒྱ་')\n",
      "19.\t('syl', 'མཚོར་')\n",
      "20.\t('syl', 'གནས་')\n",
      "21.\t('syl', 'པའི་')\n",
      "22.\t('syl', 'ཉས་')\n",
      "23.\t('syl', 'ཆུ་')\n",
      "24.\t('syl', 'འཐུང་')\n",
      "25.\t('punct', '།། །།')\n"
     ]
    }
   ],
   "source": [
    "with_markers = chunks.get_markers(with_substrings)\n",
    "for n, chunk in enumerate(with_markers):\n",
    "    print(f'{n+1}.\\t{chunk}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - punct: chunk containing Tibetan punctuation\n",
    " - syl: chunk containing a single syllable (necessarily Tibetan characters)\n",
    " - num: chunk containing Tibetan numerals\n",
    " - non-bo: chunk containing non-Tibetan characters\n",
    " \n",
    "Note that at this point, the only thing we have is substrings of the initial string with chunk markers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bochunk.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoChunk is a chunking framework built on top of BoString (see below).\n",
    "\n",
    "Its methods are used to create chunks – groups of characters that pertain to the same category. The produced output is a binary sequence of chunks matching or not a given condition.\n",
    "\n",
    "Although each chunking method only produces two kinds of chunks (those that match and those that don't), piped chunking allows to apply complex chunking patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybo.bochunk import BoChunk\n",
    "    \n",
    "bc = BoChunk(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The available chunking methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Either Tibetan characters or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the output of chunking functions are <class 'list'> each containing 3 <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "bo_nonbo = bc.chunk_bo_chars()\n",
    "print(f'the output of chunking functions are {type(bo_nonbo)} each containing {len(bo_nonbo[0])} {type(bo_nonbo[0][0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\t(100, 0, 15)\n",
      "2.\t(101, 15, 2)\n",
      "3.\t(100, 17, 75)\n"
     ]
    }
   ],
   "source": [
    "for n, c in enumerate(bo_nonbo):\n",
    "    print(f'{n+1}.\\t{c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already seen this format in PyBoChunk, which actually only applies the chunking methods available in BoChunk in order to produce chunks corresponding to the expectations of a Tibetan reader.\n",
    "\n",
    "In a human-friendly format, it actually is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\tbo\t\t\"༆ ཤི་བཀྲ་ཤིས་  \"\n",
      "2.\tnon-bo\t\t\"tr\"\n",
      "3.\tbo\t\t\" བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།\"\n"
     ]
    }
   ],
   "source": [
    "for n, r in enumerate(bc.get_markers(bc.get_chunked(bo_nonbo))):\n",
    "    print(f'{n+1}.\\t{r[0]}\\t\\t\"{r[1]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have indeed chunked the input string is a sequence of alternatively Tibetan and non-Tibetan chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Either Tibetan punctuation or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\tpunct\t\t\"༆ \"\n",
      "2.\tnon-punct\t\t\"ཤི་བཀྲ་ཤིས་\"\n",
      "3.\tpunct\t\t\"  \"\n",
      "4.\tnon-punct\t\t\"tr\"\n",
      "5.\tpunct\t\t\" \"\n",
      "6.\tnon-punct\t\t\"བདེ་\"\n",
      "7.\tpunct\t\t\"་\"\n",
      "8.\tnon-punct\t\t\"ལེ གས\"\n",
      "9.\tpunct\t\t\"། \"\n",
      "10.\tnon-punct\t\t\"བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ\"\n",
      "11.\tpunct\t\t\"། \"\n",
      "12.\tnon-punct\t\t\"མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་\"\n",
      "13.\tpunct\t\t\"།། །།\"\n"
     ]
    }
   ],
   "source": [
    "punct_nonpunct = bc.chunk_punct()\n",
    "\n",
    "for n, p in enumerate(bc.get_markers(bc.get_chunked(punct_nonpunct))):\n",
    "    print(f'{n+1}.\\t{p[0]}\\t\\t\"{p[1]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have an alternating sequence of chunks that only cares about Tibetan punctuation.\n",
    "\n",
    "Note how \"tr\" is simply considered to be non-punct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Either Tibetan symbols or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\tnon-sym\t\t\"༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།\"\n"
     ]
    }
   ],
   "source": [
    "sym_nonsym = bc.chunk_symbol()\n",
    "\n",
    "for n, s in enumerate(bc.get_markers(bc.get_chunked(sym_nonsym))):\n",
    "    print(f'{n+1}.\\t{s[0]}\\t\\t\"{s[1]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... there were no Tibetan symbols in the input string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Either Tibetan digits or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\tnon-num\t\t\"༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་\"\n",
      "2.\tnum\t\t\"༡༢༣\"\n",
      "3.\tnon-num\t\t\"ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།\"\n"
     ]
    }
   ],
   "source": [
    "num_nonnum = bc.chunk_number()\n",
    "\n",
    "for n, m in enumerate(bc.get_markers(bc.get_chunked(num_nonnum))):\n",
    "    print(f'{n+1}.\\t{m[0]}\\t\\t\"{m[1]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have correctly identified the Tibetan digits in the input string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Either spaces or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\tnon-space\t\t\"༆\"\n",
      "2.\tspace\t\t\" \"\n",
      "3.\tnon-space\t\t\"ཤི་བཀྲ་ཤིས་\"\n",
      "4.\tspace\t\t\"  \"\n",
      "5.\tnon-space\t\t\"tr\"\n",
      "6.\tspace\t\t\" \"\n",
      "7.\tnon-space\t\t\"བདེ་་ལེ\"\n",
      "8.\tspace\t\t\" \"\n",
      "9.\tnon-space\t\t\"གས།\"\n",
      "10.\tspace\t\t\" \"\n",
      "11.\tnon-space\t\t\"བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ།\"\n",
      "12.\tspace\t\t\" \"\n",
      "13.\tnon-space\t\t\"མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།།\"\n",
      "14.\tspace\t\t\" \"\n",
      "15.\tnon-space\t\t\"།།\"\n"
     ]
    }
   ],
   "source": [
    "space_nonspace = bc.chunk_spaces()\n",
    "\n",
    "for n, s in enumerate(bc.get_markers(bc.get_chunked(space_nonspace))):\n",
    "    print(f'{n+1}.\\t{s[0]}\\t\\t\"{s[1]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Syllabify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\tsyl\t\t\"༆ ཤི་\"\n",
      "2.\tsyl\t\t\"བཀྲ་\"\n",
      "3.\tsyl\t\t\"ཤིས་\"\n",
      "4.\tsyl\t\t\"  tr བདེ་་\"\n",
      "5.\tsyl\t\t\"ལེ གས། བཀྲ་\"\n",
      "6.\tsyl\t\t\"ཤིས་\"\n",
      "7.\tsyl\t\t\"བདེ་\"\n",
      "8.\tsyl\t\t\"ལེགས་\"\n",
      "9.\tsyl\t\t\"༡༢༣ཀཀ། མཐའི་\"\n",
      "10.\tsyl\t\t\"རྒྱ་\"\n",
      "11.\tsyl\t\t\"མཚོར་\"\n",
      "12.\tsyl\t\t\"གནས་\"\n",
      "13.\tsyl\t\t\"པའི་\"\n",
      "14.\tsyl\t\t\"ཉས་\"\n",
      "15.\tsyl\t\t\"ཆུ་\"\n",
      "16.\tsyl\t\t\"འཐུང་\"\n",
      "17.\tsyl\t\t\"།། །།\"\n"
     ]
    }
   ],
   "source": [
    "syl_nonsyl = bc.syllabify()\n",
    "\n",
    "for n, s in enumerate(bc.get_markers(bc.get_chunked(syl_nonsyl))):\n",
    "    print(f'{n+1}.\\t{s[0]}\\t\\t\"{s[1]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the only chunking method that doesn't create a binary sequence of chunks matching or not a given condition. \n",
    "\n",
    "Instead, it breaks up the input it receives into syllables that are only separated by tseks(both regular and non-breaking). For this reason, syllabify takes for granted that the input it receives is only Tibetan characters that can compose a syllable.\n",
    "\n",
    "When that was the case, for example in chunks 10 to 16, the syllabation is operated as expected.\n",
    "\n",
    "Spaces, on the other hand, are allowed within a syllable because they only serve to \"beautify\" a tibetan text by visually marking the end of a clause or that of a sentence after a punctuation. So we reproduce the behavior of a Tibetan reader, who will simply bypass a space encountered anywhere else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extending the framework with additional methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these chunking methods follow a simple standardized format, making it extremely simple to create new ones to suit any specific needs. They are in turn built on top of BoString, which relies on the groups of characters that were put together to reproduce the intuition of a native Tibetan who is reading a text.\n",
    "\n",
    "So in case finer chunking abilities are required, finer character groups should be created in BoString."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Piped Chunking: Implementing complex chunking patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying successively the chunking methods we have seen above in a specific order allows to design complex patterns for chunking a given string.\n",
    "\n",
    "We will exemplify this functionality by reproducing the chunking pipeline that PyBoChunk implements:\n",
    "\n",
    "    a. turn the input string into a sequence of alternating \"bo\" / \"non-bo\" chunks\n",
    "    b. turn the \"bo\" chunks into a sequence of alternating \"punct\" / \"bo\" chunks\n",
    "    c. turn the remaining \"bo\" chunks into a sequence of alternating \"sym\" / \"bo\" chunks\n",
    "    d. turn the remaining \"bo\" chunks into a sequence of alternating \"num\" / \"bo\" chunks\n",
    "    e. turn the \"bo\" chunks into a sequence of syllables\n",
    "    f. concatenate the chunks containing only spaces to the preceding one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Either \"bo\" or \"non-bo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།\n",
      "\n",
      "1.\tbo\t\t\"༆ ཤི་བཀྲ་ཤིས་  \"\n",
      "2.\tnon-bo\t\t\"tr\"\n",
      "3.\tbo\t\t\" བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།\"\n"
     ]
    }
   ],
   "source": [
    "chunks = bc.chunk_bo_chars()\n",
    "\n",
    "print('\\t', input_str, end='\\n\\n')\n",
    "for n, c in enumerate(bc.get_markers(bc.get_chunked(chunks))):\n",
    "    print(f'{n+1}.\\t{c[0]}\\t\\t\"{c[1]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is similar to what we had above. Yet this time, instead of rechunking the whole input string in \"punct\" or \"non-punct\", we will use piped chunking to only apply chunk_punct() to the \"bo\" chunks.\n",
    "\n",
    "Important: piped chunking is only possible when the content of the chunks is a tuple of ints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Either \"punct\" or \"bo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།\n",
      "\n",
      "1.\tpunct\t\t\"༆ \"\n",
      "2.\tbo\t\t\"ཤི་བཀྲ་ཤིས་\"\n",
      "3.\tpunct\t\t\"  \"\n",
      "4.\tnon-bo\t\t\"tr\"\n",
      "5.\tpunct\t\t\" \"\n",
      "6.\tbo\t\t\"བདེ་\"\n",
      "7.\tpunct\t\t\"་\"\n",
      "8.\tbo\t\t\"ལེ གས\"\n",
      "9.\tpunct\t\t\"། \"\n",
      "10.\tbo\t\t\"བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ\"\n",
      "11.\tpunct\t\t\"། \"\n",
      "12.\tbo\t\t\"མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་\"\n",
      "13.\tpunct\t\t\"།། །།\"\n"
     ]
    }
   ],
   "source": [
    "bc.pipe_chunk(chunks, bc.chunk_punct, to_chunk=bc.BO_MARKER, yes=bc.PUNCT_MARKER)\n",
    "\n",
    "print('\\t', input_str, end='\\n\\n')\n",
    "for n, c in enumerate(bc.get_markers(bc.get_chunked(chunks))):\n",
    "    print(f'{n+1}.\\t{c[0]}\\t\\t\"{c[1]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipe_chunk() takes as arguments:\n",
    " - chunks: the chunks produced by a previous chunking methods. They are expected to be a list of tuples containing each three ints.\n",
    " - bc.chunk_punct: the chunking method we want to apply on top of the existing chunks\n",
    " - to_chunk: the marker that identifies which chunks should be further processed by the new chunking method\n",
    " - yes: the marker to be used on the new chunks that match the new chunking method. (the chunks that don't match simply retain their previous marker)\n",
    "\n",
    "So to put it into simple English: \n",
    "\n",
    "                  \"Within the existing chunks, I would like to take those marked as 'bo' \n",
    "                  and within them separate what is actual Tibetan text ('bo') \n",
    "                  from what is Tibetan punctuation ('punct')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Either \"sym\" or \"bo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།\n",
      "\n",
      "1.\tpunct\t\t\"༆ \"\n",
      "2.\tbo\t\t\"ཤི་བཀྲ་ཤིས་\"\n",
      "3.\tpunct\t\t\"  \"\n",
      "4.\tnon-bo\t\t\"tr\"\n",
      "5.\tpunct\t\t\" \"\n",
      "6.\tbo\t\t\"བདེ་\"\n",
      "7.\tpunct\t\t\"་\"\n",
      "8.\tbo\t\t\"ལེ གས\"\n",
      "9.\tpunct\t\t\"། \"\n",
      "10.\tbo\t\t\"བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ\"\n",
      "11.\tpunct\t\t\"། \"\n",
      "12.\tbo\t\t\"མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་\"\n",
      "13.\tpunct\t\t\"།། །།\"\n"
     ]
    }
   ],
   "source": [
    "bc.pipe_chunk(chunks, bc.chunk_symbol, to_chunk=bc.BO_MARKER, yes=bc.SYMBOL_MARKER)\n",
    "\n",
    "print('\\t', input_str, end='\\n\\n')\n",
    "for n, c in enumerate(bc.get_markers(bc.get_chunked(chunks))):\n",
    "    print(f'{n+1}.\\t{c[0]}\\t\\t\"{c[1]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were no symbols, so the chunks are unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Either \"num\" or \"bo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།\n",
      "\n",
      "1.\tpunct\t\t\"༆ \"\n",
      "2.\tbo\t\t\"ཤི་བཀྲ་ཤིས་\"\n",
      "3.\tpunct\t\t\"  \"\n",
      "4.\tnon-bo\t\t\"tr\"\n",
      "5.\tpunct\t\t\" \"\n",
      "6.\tbo\t\t\"བདེ་\"\n",
      "7.\tpunct\t\t\"་\"\n",
      "8.\tbo\t\t\"ལེ གས\"\n",
      "9.\tpunct\t\t\"། \"\n",
      "10.\tbo\t\t\"བཀྲ་ཤིས་བདེ་ལེགས་\"\n",
      "11.\tnum\t\t\"༡༢༣\"\n",
      "12.\tbo\t\t\"ཀཀ\"\n",
      "13.\tpunct\t\t\"། \"\n",
      "14.\tbo\t\t\"མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་\"\n",
      "15.\tpunct\t\t\"།། །།\"\n"
     ]
    }
   ],
   "source": [
    "bc.pipe_chunk(chunks, bc.chunk_number, to_chunk=bc.BO_MARKER, yes=bc.NUMBER_MARKER)\n",
    "\n",
    "print('\\t', input_str, end='\\n\\n')\n",
    "for n, c in enumerate(bc.get_markers(bc.get_chunked(chunks))):\n",
    "    print(f'{n+1}.\\t{c[0]}\\t\\t\"{c[1]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                  \"Within the existing chunks, I would like to take those marked as 'bo' \n",
    "                  and within them separate what is actual Tibetan text ('bo') \n",
    "                  from what is Tibetan digits ('num')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. Syllabify \"bo\" into \"syl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།\n",
      "\n",
      "1.\tpunct\t\t\"༆ \"\n",
      "2.\tsyl\t\t\"ཤི་\"\n",
      "3.\tsyl\t\t\"བཀྲ་\"\n",
      "4.\tsyl\t\t\"ཤིས་\"\n",
      "5.\tpunct\t\t\"  \"\n",
      "6.\tnon-bo\t\t\"tr\"\n",
      "7.\tpunct\t\t\" \"\n",
      "8.\tsyl\t\t\"བདེ་\"\n",
      "9.\tpunct\t\t\"་\"\n",
      "10.\tsyl\t\t\"ལེ གས\"\n",
      "11.\tpunct\t\t\"། \"\n",
      "12.\tsyl\t\t\"བཀྲ་\"\n",
      "13.\tsyl\t\t\"ཤིས་\"\n",
      "14.\tsyl\t\t\"བདེ་\"\n",
      "15.\tsyl\t\t\"ལེགས་\"\n",
      "16.\tnum\t\t\"༡༢༣\"\n",
      "17.\tsyl\t\t\"ཀཀ\"\n",
      "18.\tpunct\t\t\"། \"\n",
      "19.\tsyl\t\t\"མཐའི་\"\n",
      "20.\tsyl\t\t\"རྒྱ་\"\n",
      "21.\tsyl\t\t\"མཚོར་\"\n",
      "22.\tsyl\t\t\"གནས་\"\n",
      "23.\tsyl\t\t\"པའི་\"\n",
      "24.\tsyl\t\t\"ཉས་\"\n",
      "25.\tsyl\t\t\"ཆུ་\"\n",
      "26.\tsyl\t\t\"འཐུང་\"\n",
      "27.\tpunct\t\t\"།། །།\"\n"
     ]
    }
   ],
   "source": [
    "bc.pipe_chunk(chunks, bc.syllabify, to_chunk=bc.BO_MARKER, yes=bc.SYL_MARKER)\n",
    "\n",
    "print('\\t', input_str, end='\\n\\n')\n",
    "for n, c in enumerate(bc.get_markers(bc.get_chunked(chunks))):\n",
    "    print(f'{n+1}.\\t{c[0]}\\t\\t\"{c[1]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are ! We have successfully preprocessed the input string into a sequence of identified content that we can confidently use for any further NLP processing.\n",
    "\n",
    "In order to have the exact same output as PyBoChunk, chunk 5 should be attached to chunk 4 and chunk 7 to chunk 6. Since this is implemented as a private method of PyBoChunk, we will stop this demonstration here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bostring.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the foundational building block of all the preprocessing classes in pybo.\n",
    "\n",
    "The idea behind it is to reproduce the way a Tibetan reader will group characters in a given text when reading it.\n",
    "This is simply done by splitting in such groups all the Unicode characters found in the Tibetan Unicode tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybo import BoString\n",
    "\n",
    "bs = BoString(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Character groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the groups we identified: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consonants:\n",
      "\t['ཀ', 'ཁ', 'ག', 'ང', 'ཅ', 'ཆ', 'ཇ', 'ཉ', 'ཏ', 'ཐ', 'ད', 'ན', 'པ', 'ཕ', 'བ', 'མ', 'ཙ', 'ཚ', 'ཛ', 'ཝ', 'ཞ', 'ཟ', 'འ', 'ཡ', 'ར', 'ལ', 'ཤ', 'ས', 'ཧ', 'ཨ', 'ཪ']\n",
      "The subscripted variants:\n",
      "\t['ྐ', 'ྒ', 'ྔ', 'ྕ', 'ྗ', 'ྙ', 'ྟ', 'ྡ', 'ྣ', 'ྤ', 'ྦ', 'ྨ', 'ྩ', 'ྫ', 'ྭ', 'ྱ', 'ྲ', 'ླ', 'ྷ']\n",
      "\n",
      "Vowels:\n",
      "\t['ི', 'ུ', 'ེ', 'ོ']\n",
      "\n",
      "Tseks (regular and non-breaking):\n",
      "\t['་', '༌']\n",
      "\n",
      "Consonants specific to Sanskrit:\n",
      "\t['གྷ', 'ཊ', 'ཋ', 'ཌ', 'ཌྷ', 'ཎ', 'དྷ', 'བྷ', 'ཛྷ', 'ཥ', 'ཀྵ', '྅']\n",
      "\n",
      "The subscripted variants of Sanskrit consonants:\n",
      "\t['ྑ', 'ྖ', 'ྠ', 'ྥ', 'ྪ', 'ྮ', 'ྯ', 'ྰ', 'ྴ', 'ྶ', 'ྸ', 'ྺ', 'ྻ', 'ྼ', 'ཱ', 'ྒྷ', 'ྚ', 'ྛ', 'ྜ', 'ྜྷ', 'ྞ', 'ྡྷ', 'ྦྷ', 'ྫྷ', 'ྵ', 'ྐྵ']\n",
      "\n",
      "Vowels specific to Sanskrit:\n",
      "\t['ཱི', 'ཱུ', 'ྲྀ', 'ཷ', 'ླྀ', 'ཹ', 'ཻ', 'ཽ', 'ྀ', 'ཱྀ', 'ྂ', 'ྃ', '྄', '྆']\n",
      "\n",
      "Sanskrit long vowel marker:\n",
      "\t['ཿ']\n",
      "\n",
      "Regular Tibetan punctuation:\n",
      "\t['༄', '༅', '༆', '༈', '།', '༎', '༏', '༐', '༑', '༔', '༴', '༼', '༽']\n",
      "\n",
      "Special Tibetan punctuation:\n",
      "\t['༁', '༂', '༃', '༒', '༇', '༉', '༊', '༺', '༻', '༾', '༿', '࿐', '࿑', '࿓', '࿔']\n",
      "\n",
      "Tibetan numerals:\n",
      "\t['༠', '༡', '༢', '༣', '༤', '༥', '༦', '༧', '༨', '༩']\n",
      "\n",
      "Markers found inside Tibetan syllables:\n",
      "\t['༵', '༷', '༸', 'ཾ']\n",
      "\n",
      "Unicode Tibetan symbols:\n",
      "\t['ༀ', '༓', '༕', '༖', '༗', '༘', '༙', '༚', '༛', '༜', '༝', '༞', '༟', '༪', '༫', '༬', '༭', '༮', '༯', '༰', '༱', '༲', '༳', '༶', '༹', '྇', 'ྈ', 'ྉ', 'ྊ', 'ྋ', 'ྌ', 'ྍ', 'ྎ', 'ྏ', '྾', '྿', '࿀', '࿁', '࿂', '࿃', '࿄', '࿅', '࿆', '࿇', '࿈', '࿉', '࿊', '࿋', '࿌', '࿎', '࿏', '࿒', '࿕', '࿖', '࿗', '࿘', '࿙', '࿚']\n",
      "\n",
      "Non-Tibetan and non-Sanskrit characters in the Tibetan tables:\n",
      "\t['ཫ', 'ཬ']\n",
      "\n",
      "All the spaces found in the Unicode tables:\n",
      "\t['\\t', ' ', ' ', '\\u180e', '\\u2000', '\\u2001', '\\u2002', '\\u2003', '\\u2004', '\\u2005', '\\u2006', '\\u2007', '\\u2008', '\\u2009', '\\u200a', '\\u200b', '\\u202f', '\\u205f', '\\u3000', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "print(f'Consonants:\\n\\t{list(bs.cons)}')\n",
    "print(f'The subscripted variants:\\n\\t{list(bs.sub_cons)}\\n')\n",
    "print(f'Vowels:\\n\\t{list(bs.vow)}\\n')\n",
    "print(f'Tseks (regular and non-breaking):\\n\\t{list(bs.tsek)}\\n')\n",
    "print(f'Consonants specific to Sanskrit:\\n\\t{list(bs.skrt_cons)}\\n')\n",
    "print(f'The subscripted variants of Sanskrit consonants:\\n\\t{list(bs.skrt_sub_cons)}\\n')\n",
    "print(f'Vowels specific to Sanskrit:\\n\\t{list(bs.skrt_vow)}\\n')\n",
    "print(f'Sanskrit long vowel marker:\\n\\t{list(bs.skrt_long_vow)}\\n')\n",
    "print(f'Regular Tibetan punctuation:\\n\\t{list(bs.normal_punct)}\\n')\n",
    "print(f'Special Tibetan punctuation:\\n\\t{list(bs.special_punct)}\\n')\n",
    "print(f'Tibetan numerals:\\n\\t{list(bs.numerals)}\\n')\n",
    "print(f'Markers found inside Tibetan syllables:\\n\\t{list(bs.in_syl_marks)}\\n')\n",
    "print(f'Unicode Tibetan symbols:\\n\\t{list(bs.symbols)}\\n')\n",
    "print(f'Non-Tibetan and non-Sanskrit characters in the Tibetan tables:\\n\\t{list(bs.non_bo_non_skrt)}\\n')\n",
    "print(f'All the spaces found in the Unicode tables:\\n\\t{list(bs.spaces)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Characters specific to Sanskrit are were distinguished in order identify the unambiguously Sanskrit syllables.\n",
    "\n",
    "The long vowel marker because when it is present at the end of a syllable, no tsek is required, so it also behaves like a syllable separator, and we need to have that information while syllabifying Tibetan text.\n",
    "\n",
    "Tibetan regular punctuation was separated from the rest thinking this might be handy when we want to do things like normalizing the punctuation. Otherwise, we could simply have one group for punctuation.\n",
    "\n",
    "Markers found inside syllables needed to be identified in order to ignore it when we want to compare a word from the dictionary and the same word in its marked form.\n",
    "\n",
    "Symbols have all been put together because we don't see yet use-cases requiring finer groups.\n",
    "\n",
    "All the space characters from the Unicode tables have been added in order to ensure we cover all possibilities and the tab character as well, because it is sometimes used instead of a space in Tibetan text.\n",
    "\n",
    "We expect that as we start using pybo for more and more things, unforseen use-cases will appear and these groups will certainly need to be refined or adjusted, but it is pretty straight forward to do so. \n",
    "\n",
    "For example, the group of Tibetan consonants is encoded in the following class attributes:\n",
    " - `self.cons = \"ཀཁགངཅཆཇཉཏཐདནཔཕབམཙཚཛཝཞཟའཡརལཤསཧཨཪ\"` (the actual characters)\n",
    " - `self.CONS = 1` (the group marker)\n",
    " - `self.char_markers = {self.CONS: 'cons', (...)}` (a human-readable replacement for the group marker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BoString's output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing BoString does is to loop once over the string and fill the `BoString.base_structure` dict with `{index: group_marker}` items for every character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 8, 1: 15, 2: 1, 3: 3, 4: 4, 5: 1, 6: 1, 7: 2, 8: 4, 9: 1, 10: 3, 11: 1, 12: 4, 13: 15, 14: 15, 15: 14, 16: 14, 17: 15, 18: 1, 19: 1, 20: 3, 21: 4, 22: 4, 23: 1, 24: 3, 25: 15, 26: 1, 27: 1, 28: 8, 29: 15, 30: 1, 31: 1, 32: 2, 33: 4, 34: 1, 35: 3, 36: 1, 37: 4, 38: 1, 39: 1, 40: 3, 41: 4, 42: 1, 43: 3, 44: 1, 45: 1, 46: 4, 47: 9, 48: 9, 49: 9, 50: 1, 51: 1, 52: 8, 53: 15, 54: 1, 55: 1, 56: 1, 57: 3, 58: 4, 59: 1, 60: 2, 61: 2, 62: 4, 63: 1, 64: 1, 65: 3, 66: 1, 67: 4, 68: 1, 69: 1, 70: 1, 71: 4, 72: 1, 73: 1, 74: 3, 75: 4, 76: 1, 77: 1, 78: 4, 79: 1, 80: 3, 81: 4, 82: 1, 83: 1, 84: 3, 85: 1, 86: 4, 87: 8, 88: 8, 89: 15, 90: 8, 91: 8}\n"
     ]
    }
   ],
   "source": [
    "print(bs.base_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same information in a human-readable form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\"༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།\"\n",
      "\n",
      "1.\t༆\tpunct\n",
      "2.\t \tspace\n",
      "3.\tཤ\tcons\n",
      "4.\tི\tvow\n",
      "5.\t་\ttsek\n",
      "6.\tབ\tcons\n",
      "7.\tཀ\tcons\n",
      "8.\tྲ\tsub-cons\n",
      "9.\t་\ttsek\n",
      "10.\tཤ\tcons\n",
      "11.\tི\tvow\n",
      "12.\tས\tcons\n",
      "13.\t་\ttsek\n",
      "14.\t \tspace\n",
      "15.\t \tspace\n",
      "16.\tt\tother\n",
      "17.\tr\tother\n",
      "18.\t \tspace\n",
      "19.\tབ\tcons\n",
      "20.\tད\tcons\n",
      "21.\tེ\tvow\n",
      "22.\t་\ttsek\n",
      "23.\t་\ttsek\n",
      "24.\tལ\tcons\n",
      "25.\tེ\tvow\n",
      "26.\t \tspace\n",
      "27.\tག\tcons\n",
      "28.\tས\tcons\n",
      "29.\t།\tpunct\n",
      "30.\t \tspace\n",
      "31.\tབ\tcons\n",
      "32.\tཀ\tcons\n",
      "33.\tྲ\tsub-cons\n",
      "34.\t་\ttsek\n",
      "35.\tཤ\tcons\n",
      "36.\tི\tvow\n",
      "37.\tས\tcons\n",
      "38.\t་\ttsek\n",
      "39.\tབ\tcons\n",
      "40.\tད\tcons\n",
      "41.\tེ\tvow\n",
      "42.\t་\ttsek\n",
      "43.\tལ\tcons\n",
      "44.\tེ\tvow\n",
      "45.\tག\tcons\n",
      "46.\tས\tcons\n",
      "47.\t་\ttsek\n",
      "48.\t༡\tnum\n",
      "49.\t༢\tnum\n",
      "50.\t༣\tnum\n",
      "51.\tཀ\tcons\n",
      "52.\tཀ\tcons\n",
      "53.\t།\tpunct\n",
      "54.\t \tspace\n",
      "55.\tམ\tcons\n",
      "56.\tཐ\tcons\n",
      "57.\tའ\tcons\n",
      "58.\tི\tvow\n",
      "59.\t་\ttsek\n",
      "60.\tར\tcons\n",
      "61.\tྒ\tsub-cons\n",
      "62.\tྱ\tsub-cons\n",
      "63.\t་\ttsek\n",
      "64.\tམ\tcons\n",
      "65.\tཚ\tcons\n",
      "66.\tོ\tvow\n",
      "67.\tར\tcons\n",
      "68.\t་\ttsek\n",
      "69.\tག\tcons\n",
      "70.\tན\tcons\n",
      "71.\tས\tcons\n",
      "72.\t་\ttsek\n",
      "73.\tཔ\tcons\n",
      "74.\tའ\tcons\n",
      "75.\tི\tvow\n",
      "76.\t་\ttsek\n",
      "77.\tཉ\tcons\n",
      "78.\tས\tcons\n",
      "79.\t་\ttsek\n",
      "80.\tཆ\tcons\n",
      "81.\tུ\tvow\n",
      "82.\t་\ttsek\n",
      "83.\tའ\tcons\n",
      "84.\tཐ\tcons\n",
      "85.\tུ\tvow\n",
      "86.\tང\tcons\n",
      "87.\t་\ttsek\n",
      "88.\t།\tpunct\n",
      "89.\t།\tpunct\n",
      "90.\t \tspace\n",
      "91.\t།\tpunct\n",
      "92.\t།\tpunct\n"
     ]
    }
   ],
   "source": [
    "print(f'\\n\\t\"{input_str}\"\\n')\n",
    "for idx, g in bs.base_structure.items():\n",
    "    character = input_str[idx]\n",
    "    group = bs.char_markers[g]\n",
    "    print(f'{idx+1}.\\t{character}\\t{group}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What might come in handy is the ability to export the groups for a portion of the input string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'cons', 1: 'vow', 2: 'tsek', 3: 'cons', 4: 'cons'}\n"
     ]
    }
   ],
   "source": [
    "start = 2\n",
    "end = 5\n",
    "sub_str = bs.export_groups(start, end)\n",
    "\n",
    "# now fetching the human-readable markers\n",
    "sub_str = {idx: bs.char_markers[group] for idx, group in sub_str.items()}\n",
    "print(sub_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the indices in `sub_str` are adjusted for the substring.\n",
    "\n",
    "In case we want to keep the original indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 'cons', 3: 'vow', 4: 'tsek', 5: 'cons', 6: 'cons'}\n"
     ]
    }
   ],
   "source": [
    "orig_indices = bs.export_groups(start, end, for_substring=False)\n",
    "\n",
    "print({idx: bs.char_markers[group] for idx, group in orig_indices.items()})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
