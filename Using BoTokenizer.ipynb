{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PyBo's Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Running the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybo import BoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciate the tokenizer with the 'POS' profile (see [profile documentation](this.file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Trie...\n",
      "Time: 2.4239799976348877\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BoTokenizer('POS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a random text in Tibetan language,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = '༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།མཁའ།'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what information can be derived from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output is a <class 'list'>.\n",
      "The constituting elements are <class 'pybo.token.Token'>s.\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(input_str)\n",
    "print(f'The output is a {type(tokens)}.\\nThe constituting elements are {type(tokens[0])}s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing without separating affixed particles is also possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_split = tokenizer.tokenize(input_str, split_affixes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. A first look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non Tibetan tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing, I see there is non-Tibetan stuff in the middle of the input string. Let's see how I can detect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"tr \", token number 4, is not Tibetan.\n",
      "this starts at 15th character in the input and spans 3 characters\n"
     ]
    }
   ],
   "source": [
    "for n, token in enumerate(tokens):\n",
    "    if token.type == 'non-bo':\n",
    "        content = token.content\n",
    "        print(f'\"{content}\", token number {n+1}, is not Tibetan.')\n",
    "        start = token.start\n",
    "        length = token.len\n",
    "        print(f'this starts at {start}th character in the input and spans {length} characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokens that are not words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there any Tibetan punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"༆ \", token number 1, is a punctuation token.\n",
      "\"། \", token number 6, is a punctuation token.\n",
      "\"། \", token number 11, is a punctuation token.\n",
      "\"།། །།\", token number 22, is a punctuation token.\n",
      "\"།\", token number 24, is a punctuation token.\n"
     ]
    }
   ],
   "source": [
    "for n, token in enumerate(tokens):\n",
    "    if token.type == 'punct':\n",
    "        content = token.content\n",
    "        print(f'\"{content}\", token number {n+1}, is a punctuation token.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are the Tibetan digits treated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"༡༢༣\", token number 9, is a numeral.\n"
     ]
    }
   ],
   "source": [
    "for n, token in enumerate(tokens):\n",
    "    if token.type == 'num':\n",
    "        content = token.content\n",
    "        print(f'\"{content}\", token number {n+1}, is a numeral.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting affixed particles or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting them: མཐ, འི་\n",
      "keeping them together: མཐའི་\n"
     ]
    }
   ],
   "source": [
    "print(f'splitting them: {tokens[11].content}, {tokens[12].content}')\n",
    "print(f'keeping them together: {not_split[11].content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The attributes of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strictly speaking, a token is a word that has been correctly extracted from the input string, but our Token objects have much more information that is awaiting to be exploited by NLP treatments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token.content – the unmodified content straight from the input string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།མཁའ།\"\n",
      "\n",
      "1.\t \"༆ \"\n",
      "2.\t \"ཤི་\"\n",
      "3.\t \"བཀྲ་ཤིས་  \"\n",
      "4.\t \"tr \"\n",
      "5.\t \"བདེ་་ལེ གས\"\n",
      "6.\t \"། \"\n",
      "7.\t \"བཀྲ་ཤིས་\"\n",
      "8.\t \"བདེ་ལེགས་\"\n",
      "9.\t \"༡༢༣\"\n",
      "10.\t \"ཀཀ\"\n",
      "11.\t \"། \"\n",
      "12.\t \"མཐ\"\n",
      "13.\t \"འི་\"\n",
      "14.\t \"རྒྱ་མཚོ\"\n",
      "15.\t \"ར་\"\n",
      "16.\t \"གནས་པ\"\n",
      "17.\t \"འི་\"\n",
      "18.\t \"ཉ\"\n",
      "19.\t \"ས་\"\n",
      "20.\t \"ཆུ་\"\n",
      "21.\t \"འཐུང་\"\n",
      "22.\t \"།། །།\"\n",
      "23.\t \"མཁའ\"\n",
      "24.\t \"།\"\n"
     ]
    }
   ],
   "source": [
    "print(f'\"{input_str}\"\\n')\n",
    "for n, token in enumerate(tokens):\n",
    "    print(f'{n+1}.\\t \"{token.content}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token.type – the basic types of tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།མཁའ།\"\n",
      "\n",
      "1.\tpunct\t(\"༆ \")\n",
      "2.\tsyl\t(\"ཤི་\")\n",
      "3.\tsyl\t(\"བཀྲ་ཤིས་  \")\n",
      "4.\tnon-bo\t(\"tr \")\n",
      "5.\tsyl\t(\"བདེ་་ལེ གས\")\n",
      "6.\tpunct\t(\"། \")\n",
      "7.\tsyl\t(\"བཀྲ་ཤིས་\")\n",
      "8.\tsyl\t(\"བདེ་ལེགས་\")\n",
      "9.\tnum\t(\"༡༢༣\")\n",
      "10.\tsyl\t(\"ཀཀ\")\n",
      "11.\tpunct\t(\"། \")\n",
      "12.\tsyl\t(\"མཐ\")\n",
      "13.\tsyl\t(\"འི་\")\n",
      "14.\tsyl\t(\"རྒྱ་མཚོ\")\n",
      "15.\tsyl\t(\"ར་\")\n",
      "16.\tsyl\t(\"གནས་པ\")\n",
      "17.\tsyl\t(\"འི་\")\n",
      "18.\tsyl\t(\"ཉ\")\n",
      "19.\tsyl\t(\"ས་\")\n",
      "20.\tsyl\t(\"ཆུ་\")\n",
      "21.\tsyl\t(\"འཐུང་\")\n",
      "22.\tpunct\t(\"།། །།\")\n",
      "23.\tsyl\t(\"མཁའ\")\n",
      "24.\tpunct\t(\"།\")\n"
     ]
    }
   ],
   "source": [
    "print(f'\"{input_str}\"\\n')\n",
    "for n, token in enumerate(tokens):\n",
    "    print(f'{n+1}.\\t{token.type}\\t(\"{token.content}\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - syl: contains valid Tibetan syllables\n",
    " - num: Tibetan numerals\n",
    " - punct: Tibetan punctuation\n",
    " - non-bo: non-Tibetan content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token.pos – Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།མཁའ།\"\n",
      "\n",
      "1.\tpunct\t\t(\"༆ \")\n",
      "2.\tVERB\t\t(\"ཤི་\")\n",
      "3.\tNOUN\t\t(\"བཀྲ་ཤིས་  \")\n",
      "4.\tnon-bo\t\t(\"tr \")\n",
      "5.\tNOUN\t\t(\"བདེ་་ལེ གས\")\n",
      "6.\tpunct\t\t(\"། \")\n",
      "7.\tNOUN\t\t(\"བཀྲ་ཤིས་\")\n",
      "8.\tNOUN\t\t(\"བདེ་ལེགས་\")\n",
      "9.\tnum\t\t(\"༡༢༣\")\n",
      "10.\tnon-word\t\t(\"ཀཀ\")\n",
      "11.\tpunct\t\t(\"། \")\n",
      "12.\tNOUN\t\t(\"མཐ\")\n",
      "13.\tPART\t\t(\"འི་\")\n",
      "14.\tNOUN\t\t(\"རྒྱ་མཚོ\")\n",
      "15.\tPART\t\t(\"ར་\")\n",
      "16.\tVERB\t\t(\"གནས་པ\")\n",
      "17.\tPART\t\t(\"འི་\")\n",
      "18.\tNOUN\t\t(\"ཉ\")\n",
      "19.\tPART\t\t(\"ས་\")\n",
      "20.\tNOUN\t\t(\"ཆུ་\")\n",
      "21.\toov\t\t(\"འཐུང་\")\n",
      "22.\tpunct\t\t(\"།། །།\")\n",
      "23.\tNOUN\t\t(\"མཁའ\")\n",
      "24.\tpunct\t\t(\"།\")\n"
     ]
    }
   ],
   "source": [
    "print(f'\"{input_str}\"\\n')\n",
    "for n, token in enumerate(tokens):\n",
    "    print(f'{n+1}.\\t{token.pos}\\t\\t(\"{token.content}\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - NOUN:    Tibetan noun\n",
    " - VERB:    Tibetan verb\n",
    " - PART:    casual particle (affixed or not)\n",
    " - oov:     Tibetan word for which no POS was found\n",
    " - non-word:A sequence of Tibetan letters that does not appear in our list of words\n",
    " \n",
    " \n",
    " - punct:   Tibetan punctuation\n",
    " - num:     Tibetan numerals\n",
    " - non-bo:  non-Tibetan characters (spaces have a special treatment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token.tag – Token.pos augmented with morphological information on affixed particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།མཁའ།\"\n",
      "\n",
      "1.\tpunct\t\t(\"༆ \")\n",
      "2.\tVERBᛃᛃᛃ\t\t(\"ཤི་\")\n",
      "3.\tNOUNᛃᛃᛃ\t\t(\"བཀྲ་ཤིས་  \")\n",
      "4.\tnon-bo\t\t(\"tr \")\n",
      "5.\tNOUNᛃᛃᛃ\t\t(\"བདེ་་ལེ གས\")\n",
      "6.\tpunct\t\t(\"། \")\n",
      "7.\tNOUNᛃᛃᛃ\t\t(\"བཀྲ་ཤིས་\")\n",
      "8.\tNOUNᛃᛃᛃ\t\t(\"བདེ་ལེགས་\")\n",
      "9.\tnum\t\t(\"༡༢༣\")\n",
      "10.\tnon-word\t\t(\"ཀཀ\")\n",
      "11.\tpunct\t\t(\"། \")\n",
      "12.\tNOUNᛃᛃᛃ\t\t(\"མཐ\")\n",
      "13.\tPARTᛃgiᛃᛃ\t\t(\"འི་\")\n",
      "14.\tNOUNᛃᛃᛃ\t\t(\"རྒྱ་མཚོ\")\n",
      "15.\tPARTᛃlaᛃᛃ\t\t(\"ར་\")\n",
      "16.\tVERBᛃᛃᛃ\t\t(\"གནས་པ\")\n",
      "17.\tPARTᛃgiᛃᛃ\t\t(\"འི་\")\n",
      "18.\tNOUNᛃᛃᛃ\t\t(\"ཉ\")\n",
      "19.\tPARTᛃgisᛃᛃ\t\t(\"ས་\")\n",
      "20.\tNOUNᛃᛃᛃ\t\t(\"ཆུ་\")\n",
      "21.\toov\t\t(\"འཐུང་\")\n",
      "22.\tpunct\t\t(\"།། །།\")\n",
      "23.\tNOUNᛃᛃᛃ\t\t(\"མཁའ\")\n",
      "24.\tpunct\t\t(\"།\")\n"
     ]
    }
   ],
   "source": [
    "print(f'\"{input_str}\"\\n')\n",
    "for n, token in enumerate(tokens):\n",
    "    print(f'{n+1}.\\t{token.tag}\\t\\t(\"{token.content}\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - la: the ladon(ལ་དོན་) particle was affixed to the previous token\n",
    " - gi: the dreldra(འབྲེལ་སྒྲ་) particle was affixed\n",
    " - gis: the jedra(བྱེད་སྒྲ་) particle was affixed\n",
    "\n",
    "note: The runic character \"ᛃ\" is used as a separator because we assume it won't ever appear besides Tibetan text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token.lemma – The current word in its canonical form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།མཁའ།\"\n",
      "\n",
      "1.\t\"\"\t\t(\"༆ \")\n",
      "2.\t\"ཤི་\"\t\t(\"ཤི་\")\n",
      "3.\t\"བཀྲ་ཤིས་\"\t\t(\"བཀྲ་ཤིས་  \")\n",
      "4.\t\"\"\t\t(\"tr \")\n",
      "5.\t\"བདེ་ལེགས་\"\t\t(\"བདེ་་ལེ གས\")\n",
      "6.\t\"\"\t\t(\"། \")\n",
      "7.\t\"བཀྲ་ཤིས་\"\t\t(\"བཀྲ་ཤིས་\")\n",
      "8.\t\"བདེ་ལེགས་\"\t\t(\"བདེ་ལེགས་\")\n",
      "9.\t\"\"\t\t(\"༡༢༣\")\n",
      "10.\t\"ཀཀ་\"\t\t(\"ཀཀ\")\n",
      "11.\t\"\"\t\t(\"། \")\n",
      "12.\t\"མཐའ་\"\t\t(\"མཐ\")\n",
      "13.\t\"གི་\"\t\t(\"འི་\")\n",
      "14.\t\"རྒྱ་མཚོ་\"\t\t(\"རྒྱ་མཚོ\")\n",
      "15.\t\"ལ་\"\t\t(\"ར་\")\n",
      "16.\t\"གནས་པ་\"\t\t(\"གནས་པ\")\n",
      "17.\t\"གི་\"\t\t(\"འི་\")\n",
      "18.\t\"ཉ་\"\t\t(\"ཉ\")\n",
      "19.\t\"གིས་\"\t\t(\"ས་\")\n",
      "20.\t\"ཆུ་\"\t\t(\"ཆུ་\")\n",
      "21.\t\"འཐུང་\"\t\t(\"འཐུང་\")\n",
      "22.\t\"\"\t\t(\"།། །།\")\n",
      "23.\t\"མཁའ་\"\t\t(\"མཁའ\")\n",
      "24.\t\"\"\t\t(\"།\")\n"
     ]
    }
   ],
   "source": [
    "print(f'\"{input_str}\"\\n')\n",
    "for n, token in enumerate(tokens):\n",
    "    print(f'{n+1}.\\t\"{token.lemma}\"\\t\\t(\"{token.content}\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only tokens with  have some content in this attribute. The other ones have an empty string.\n",
    "\n",
    "\n",
    "Token 13 is a ladon(ལ་དོན་) particle that is affixed, so its lemma is the canonical form of this casual particle: \"ལ་\". The same goes for token 15 and 17. \n",
    "\n",
    "\n",
    "The final འ is reconstructed where necessary (token 12).\n",
    "\n",
    "\n",
    "When we have a lemma for a given word in our list, we provide it, such as for token 5, but otherwise, we chose to give the normalized version of the content, such as in token 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token.cleaned_content – the normalized form of Token.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།མཁའ།\"\n",
      "\n",
      "1.\t\"\"\t\t(\"༆ \")\n",
      "2.\t\"ཤི་\"\t\t(\"ཤི་\")\n",
      "3.\t\"བཀྲ་ཤིས་\"\t\t(\"བཀྲ་ཤིས་  \")\n",
      "4.\t\"\"\t\t(\"tr \")\n",
      "5.\t\"བདེ་ལེགས་\"\t\t(\"བདེ་་ལེ གས\")\n",
      "6.\t\"\"\t\t(\"། \")\n",
      "7.\t\"བཀྲ་ཤིས་\"\t\t(\"བཀྲ་ཤིས་\")\n",
      "8.\t\"བདེ་ལེགས་\"\t\t(\"བདེ་ལེགས་\")\n",
      "9.\t\"\"\t\t(\"༡༢༣\")\n",
      "10.\t\"ཀཀ་\"\t\t(\"ཀཀ\")\n",
      "11.\t\"\"\t\t(\"། \")\n",
      "12.\t\"མཐ\"\t\t(\"མཐ\")\n",
      "13.\t\"འི་\"\t\t(\"འི་\")\n",
      "14.\t\"རྒྱ་མཚོ\"\t\t(\"རྒྱ་མཚོ\")\n",
      "15.\t\"ར་\"\t\t(\"ར་\")\n",
      "16.\t\"གནས་པ\"\t\t(\"གནས་པ\")\n",
      "17.\t\"འི་\"\t\t(\"འི་\")\n",
      "18.\t\"ཉ\"\t\t(\"ཉ\")\n",
      "19.\t\"ས་\"\t\t(\"ས་\")\n",
      "20.\t\"ཆུ་\"\t\t(\"ཆུ་\")\n",
      "21.\t\"འཐུང་\"\t\t(\"འཐུང་\")\n",
      "22.\t\"\"\t\t(\"།། །།\")\n",
      "23.\t\"མཁའ་\"\t\t(\"མཁའ\")\n",
      "24.\t\"\"\t\t(\"།\")\n"
     ]
    }
   ],
   "source": [
    "print(f'\"{input_str}\"\\n')\n",
    "for n, token in enumerate(tokens):\n",
    "    print(f'{n+1}.\\t\"{token.cleaned_content}\"\\t\\t(\"{token.content}\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The different Unicode spaces and tabs are removed, \n",
    "2. Insecable tseks are replaced with regular tseks.\n",
    "2. Tseks are added at the end of every syllable (not at the end of every token)\n",
    "\n",
    "\n",
    "See for example in token 5 that the double tsek is reduced and that a tsek is added at the end of the second syllable.\n",
    "On the other hand, tokens 12, 14 and 16 don't end with a tsek since their last syllable ends in the following token.\n",
    "\n",
    "note: as of now, the normalization of punctuation is not implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token.unaffixed_word – Token.cleaned_content augmented with the འ reinsertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།མཁའ།\"\n",
      "\n",
      "1.\t\"\"\t\t(\"༆ \")\n",
      "2.\t\"ཤི་\"\t\t(\"ཤི་\")\n",
      "3.\t\"བཀྲ་ཤིས་\"\t\t(\"བཀྲ་ཤིས་  \")\n",
      "4.\t\"\"\t\t(\"tr \")\n",
      "5.\t\"བདེ་ལེགས་\"\t\t(\"བདེ་་ལེ གས\")\n",
      "6.\t\"\"\t\t(\"། \")\n",
      "7.\t\"བཀྲ་ཤིས་\"\t\t(\"བཀྲ་ཤིས་\")\n",
      "8.\t\"བདེ་ལེགས་\"\t\t(\"བདེ་ལེགས་\")\n",
      "9.\t\"\"\t\t(\"༡༢༣\")\n",
      "10.\t\"ཀཀ་\"\t\t(\"ཀཀ\")\n",
      "11.\t\"\"\t\t(\"། \")\n",
      "12.\t\"མཐའ་\"\t\t(\"མཐ\")\n",
      "13.\t\"འི་\"\t\t(\"འི་\")\n",
      "14.\t\"རྒྱ་མཚོ་\"\t\t(\"རྒྱ་མཚོ\")\n",
      "15.\t\"ར་\"\t\t(\"ར་\")\n",
      "16.\t\"གནས་པ་\"\t\t(\"གནས་པ\")\n",
      "17.\t\"འི་\"\t\t(\"འི་\")\n",
      "18.\t\"ཉ་\"\t\t(\"ཉ\")\n",
      "19.\t\"ས་\"\t\t(\"ས་\")\n",
      "20.\t\"ཆུ་\"\t\t(\"ཆུ་\")\n",
      "21.\t\"འཐུང་\"\t\t(\"འཐུང་\")\n",
      "22.\t\"\"\t\t(\"།། །།\")\n",
      "23.\t\"མཁའ་\"\t\t(\"མཁའ\")\n",
      "24.\t\"\"\t\t(\"།\")\n"
     ]
    }
   ],
   "source": [
    "print(f'\"{input_str}\"\\n')\n",
    "for n, token in enumerate(tokens):\n",
    "    print(f'{n+1}.\\t\"{token.unaffixed_word}\"\\t\\t(\"{token.content}\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When tokens contain an affixed particle, the unaffixed form is reconstructed.\n",
    "འ is reinserted in token 12, but not in token 16, nor 14, nor 18.\n",
    "\n",
    "\n",
    "This also functions when we choose not to separate affixed particles from their hosting word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\t\"\"\t\t(\"༆ \")\n",
      "2.\t\"ཤི་\"\t\t(\"ཤི་\")\n",
      "3.\t\"བཀྲ་ཤིས་\"\t\t(\"བཀྲ་ཤིས་  \")\n",
      "4.\t\"\"\t\t(\"tr \")\n",
      "5.\t\"བདེ་ལེགས་\"\t\t(\"བདེ་་ལེ གས\")\n",
      "6.\t\"\"\t\t(\"། \")\n",
      "7.\t\"བཀྲ་ཤིས་\"\t\t(\"བཀྲ་ཤིས་\")\n",
      "8.\t\"བདེ་ལེགས་\"\t\t(\"བདེ་ལེགས་\")\n",
      "9.\t\"\"\t\t(\"༡༢༣\")\n",
      "10.\t\"ཀཀ་\"\t\t(\"ཀཀ\")\n",
      "11.\t\"\"\t\t(\"། \")\n",
      "12.\t\"མཐའ་\"\t\t(\"མཐའི་\")\n",
      "13.\t\"རྒྱ་མཚོ་\"\t\t(\"རྒྱ་མཚོར་\")\n",
      "14.\t\"གནས་པ་\"\t\t(\"གནས་པའི་\")\n",
      "15.\t\"ཉ་\"\t\t(\"ཉས་\")\n",
      "16.\t\"ཆུ་\"\t\t(\"ཆུ་\")\n",
      "17.\t\"འཐུང་\"\t\t(\"འཐུང་\")\n",
      "18.\t\"\"\t\t(\"།། །།\")\n",
      "19.\t\"མཁའ་\"\t\t(\"མཁའ\")\n",
      "20.\t\"\"\t\t(\"།\")\n"
     ]
    }
   ],
   "source": [
    "for n, token in enumerate(not_split):\n",
    "    print(f'{n+1}.\\t\"{token.unaffixed_word}\"\\t\\t(\"{token.content}\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token.affix & Token.affixed – Host-word and its affixed particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།མཁའ།\"\n",
      "\n",
      "1.\t\t\t(\"༆ \")\n",
      "2.\t\t\t(\"ཤི་\")\n",
      "3.\t\t\t(\"བཀྲ་ཤིས་  \")\n",
      "4.\t\t\t(\"tr \")\n",
      "5.\t\t\t(\"བདེ་་ལེ གས\")\n",
      "6.\t\t\t(\"། \")\n",
      "7.\t\t\t(\"བཀྲ་ཤིས་\")\n",
      "8.\t\t\t(\"བདེ་ལེགས་\")\n",
      "9.\t\t\t(\"༡༢༣\")\n",
      "10.\t\t\t(\"ཀཀ\")\n",
      "11.\t\t\t(\"། \")\n",
      "12.\tHost\t\t(\"མཐ\")\n",
      "13.\tAffix\t\t(\"འི་\")\n",
      "14.\tHost\t\t(\"རྒྱ་མཚོ\")\n",
      "15.\tAffix\t\t(\"ར་\")\n",
      "16.\tHost\t\t(\"གནས་པ\")\n",
      "17.\tAffix\t\t(\"འི་\")\n",
      "18.\tHost\t\t(\"ཉ\")\n",
      "19.\tAffix\t\t(\"ས་\")\n",
      "20.\t\t\t(\"ཆུ་\")\n",
      "21.\t\t\t(\"འཐུང་\")\n",
      "22.\t\t\t(\"།། །།\")\n",
      "23.\t\t\t(\"མཁའ\")\n",
      "24.\t\t\t(\"།\")\n"
     ]
    }
   ],
   "source": [
    "print(f'\"{input_str}\"\\n')\n",
    "for n, token in enumerate(tokens):\n",
    "    if token.affix: # boolean value: True\n",
    "        print(f'{n+1}.\\tAffix\\t\\t(\"{token.content}\")')\n",
    "    elif token.affixed:\n",
    "        print(f'{n+1}.\\tHost\\t\\t(\"{token.content}\")')\n",
    "    else:\n",
    "        print(f'{n+1}.\\t\\t\\t(\"{token.content}\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token.aa_word – Signals words that end with འ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།མཁའ།\"\n",
      "\n",
      "1.\t\t\t(\"༆ \")\n",
      "2.\t\t\t(\"ཤི་\")\n",
      "3.\t\t\t(\"བཀྲ་ཤིས་  \")\n",
      "4.\t\t\t(\"tr \")\n",
      "5.\t\t\t(\"བདེ་་ལེ གས\")\n",
      "6.\t\t\t(\"། \")\n",
      "7.\t\t\t(\"བཀྲ་ཤིས་\")\n",
      "8.\t\t\t(\"བདེ་ལེགས་\")\n",
      "9.\t\t\t(\"༡༢༣\")\n",
      "10.\t\t\t(\"ཀཀ\")\n",
      "11.\t\t\t(\"། \")\n",
      "12.\tTrue\t\t(\"མཐ\")\n",
      "13.\t\t\t(\"འི་\")\n",
      "14.\t\t\t(\"རྒྱ་མཚོ\")\n",
      "15.\t\t\t(\"ར་\")\n",
      "16.\t\t\t(\"གནས་པ\")\n",
      "17.\t\t\t(\"འི་\")\n",
      "18.\t\t\t(\"ཉ\")\n",
      "19.\t\t\t(\"ས་\")\n",
      "20.\t\t\t(\"ཆུ་\")\n",
      "21.\t\t\t(\"འཐུང་\")\n",
      "22.\t\t\t(\"།། །།\")\n",
      "23.\t\t\t(\"མཁའ\")\n",
      "24.\t\t\t(\"།\")\n"
     ]
    }
   ],
   "source": [
    "print(f'\"{input_str}\"\\n')\n",
    "for n, token in enumerate(tokens):\n",
    "    if token.aa_word: # boolean value: True\n",
    "        print(f'{n+1}.\\tTrue\\t\\t(\"{token.content}\")')\n",
    "    else:\n",
    "        print(f'{n+1}.\\t\\t\\t(\"{token.content}\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: This is currently not detected in words not containing affixed particles, such as token 23."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token.syls – Individual syllables of every token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"༆ ཤི་བཀྲ་ཤིས་  tr བདེ་་ལེ གས། བཀྲ་ཤིས་བདེ་ལེགས་༡༢༣ཀཀ། མཐའི་རྒྱ་མཚོར་གནས་པའི་ཉས་ཆུ་འཐུང་།། །།མཁའ།\"\n",
      "\n",
      "1.\tNone\t\t\t\t(\"༆ \")\n",
      "2.\t[[0, 1]]\t\t\t\t(\"ཤི་\")\n",
      "3.\t[[0, 1, 2], [4, 5, 6]]\t\t\t\t(\"བཀྲ་ཤིས་  \")\n",
      "4.\tNone\t\t\t\t(\"tr \")\n",
      "5.\t[[0, 1, 2], [5, 6, 8, 9]]\t\t\t\t(\"བདེ་་ལེ གས\")\n",
      "6.\tNone\t\t\t\t(\"། \")\n",
      "7.\t[[0, 1, 2], [4, 5, 6]]\t\t\t\t(\"བཀྲ་ཤིས་\")\n",
      "8.\t[[0, 1, 2], [4, 5, 6, 7]]\t\t\t\t(\"བདེ་ལེགས་\")\n",
      "9.\tNone\t\t\t\t(\"༡༢༣\")\n",
      "10.\t[[0, 1]]\t\t\t\t(\"ཀཀ\")\n",
      "11.\tNone\t\t\t\t(\"། \")\n",
      "12.\t[[0, 1, 2, 3]]\t\t\t\t(\"མཐའི་\")\n",
      "13.\t[[0, 1, 2], [4, 5, 6, 7]]\t\t\t\t(\"རྒྱ་མཚོར་\")\n",
      "14.\t[[0, 1, 2], [4, 5, 6]]\t\t\t\t(\"གནས་པའི་\")\n",
      "15.\t[[0, 1]]\t\t\t\t(\"ཉས་\")\n",
      "16.\t[[0, 1]]\t\t\t\t(\"ཆུ་\")\n",
      "17.\t[[0, 1, 2, 3]]\t\t\t\t(\"འཐུང་\")\n",
      "18.\tNone\t\t\t\t(\"།། །།\")\n",
      "19.\t[[0, 1, 2]]\t\t\t\t(\"མཁའ\")\n",
      "20.\tNone\t\t\t\t(\"།\")\n"
     ]
    }
   ],
   "source": [
    "print(f'\"{input_str}\"\\n')\n",
    "for n, token in enumerate(not_split):\n",
    "    print(f'{n+1}.\\t{token.syls}\\t\\t\\t\\t(\"{token.content}\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens containing no syllabe have \"None\" as value for this attribute.\n",
    "\n",
    "\n",
    "For the others, every syllable is represented as a list containing indices.\n",
    "\n",
    "The indices are relative to the beginning of the current token (Token.start attribute)\n",
    "Each index corresponds to a letter of the syllabe (spaces and tseks are omitted).\n",
    "\n",
    "\n",
    "Here is how we can make use of them to get a cleaned syllable using this attribute and the original string (input_str):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      "2.\tཤི་\t\t<- ['ཤི']\t\t<- [['ཤ', 'ི']]\n",
      "3.\tབཀྲ་ཤིས་\t\t<- ['བཀྲ', 'ཤིས']\t\t<- [['བ', 'ཀ', 'ྲ'], ['ཤ', 'ི', 'ས']]\n",
      "4.\n",
      "5.\tབདེ་ལེགས་\t\t<- ['བདེ', 'ལེགས']\t\t<- [['བ', 'ད', 'ེ'], ['ལ', 'ེ', 'ག', 'ས']]\n",
      "6.\n",
      "7.\tབཀྲ་ཤིས་\t\t<- ['བཀྲ', 'ཤིས']\t\t<- [['བ', 'ཀ', 'ྲ'], ['ཤ', 'ི', 'ས']]\n",
      "8.\tབདེ་ལེགས་\t\t<- ['བདེ', 'ལེགས']\t\t<- [['བ', 'ད', 'ེ'], ['ལ', 'ེ', 'ག', 'ས']]\n",
      "9.\n",
      "10.\tཀཀ་\t\t<- ['ཀཀ']\t\t<- [['ཀ', 'ཀ']]\n",
      "11.\n",
      "12.\tམཐའི་\t\t<- ['མཐའི']\t\t<- [['མ', 'ཐ', 'འ', 'ི']]\n",
      "13.\tརྒྱ་མཚོར་\t\t<- ['རྒྱ', 'མཚོར']\t\t<- [['ར', 'ྒ', 'ྱ'], ['མ', 'ཚ', 'ོ', 'ར']]\n",
      "14.\tགནས་པའི་\t\t<- ['གནས', 'པའི']\t\t<- [['ག', 'ན', 'ས'], ['པ', 'འ', 'ི']]\n",
      "15.\tཉས་\t\t<- ['ཉས']\t\t<- [['ཉ', 'ས']]\n",
      "16.\tཆུ་\t\t<- ['ཆུ']\t\t<- [['ཆ', 'ུ']]\n",
      "17.\tའཐུང་\t\t<- ['འཐུང']\t\t<- [['འ', 'ཐ', 'ུ', 'ང']]\n",
      "18.\n",
      "19.\tམཁའ་\t\t<- ['མཁའ']\t\t<- [['མ', 'ཁ', 'འ']]\n",
      "20.\n"
     ]
    }
   ],
   "source": [
    "for n, token in enumerate(not_split):\n",
    "    if token.syls:\n",
    "        syls_in_list_of_chars = []\n",
    "        for s in token.syls:\n",
    "            syls_in_list_of_chars.append([input_str[token.start + a] for a in s])\n",
    "        syls_in_list = [''.join(a) for a in syls_in_list_of_chars]\n",
    "        clean_content = '་'.join(syls_in_list) + '་'\n",
    "        print(f'{n+1}.\\t{clean_content}\\t\\t<- {syls_in_list}\\t\\t<- {syls_in_list_of_chars}')\n",
    "    else:\n",
    "        print(f'{n+1}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token.char_types – General categorization of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. '༆':punct, ' ':space, \n",
      "2. 'ཤ':cons, 'ི':vow, '་':tsek, \n",
      "3. 'བ':cons, 'ཀ':cons, 'ྲ':sub-cons, '་':tsek, 'ཤ':cons, 'ི':vow, 'ས':cons, '་':tsek, ' ':space, ' ':space, \n",
      "4. 't':other, 'r':other, ' ':space, \n",
      "5. 'བ':cons, 'ད':cons, 'ེ':vow, '་':tsek, '་':tsek, 'ལ':cons, 'ེ':vow, ' ':space, 'ག':cons, 'ས':cons, \n",
      "6. '།':punct, ' ':space, \n",
      "7. 'བ':cons, 'ཀ':cons, 'ྲ':sub-cons, '་':tsek, 'ཤ':cons, 'ི':vow, 'ས':cons, '་':tsek, \n",
      "8. 'བ':cons, 'ད':cons, 'ེ':vow, '་':tsek, 'ལ':cons, 'ེ':vow, 'ག':cons, 'ས':cons, '་':tsek, \n",
      "9. '༡':num, '༢':num, '༣':num, \n",
      "10. 'ཀ':cons, 'ཀ':cons, \n",
      "11. '།':punct, ' ':space, \n",
      "12. 'མ':cons, 'ཐ':cons, \n",
      "13. 'འ':cons, 'ི':vow, '་':tsek, \n",
      "14. 'ར':cons, 'ྒ':sub-cons, 'ྱ':sub-cons, '་':tsek, 'མ':cons, 'ཚ':cons, 'ོ':vow, \n",
      "15. 'ར':cons, '་':tsek, \n",
      "16. 'ག':cons, 'ན':cons, 'ས':cons, '་':tsek, 'པ':cons, \n",
      "17. 'འ':cons, 'ི':vow, '་':tsek, \n",
      "18. 'ཉ':cons, \n",
      "19. 'ས':cons, '་':tsek, \n",
      "20. 'ཆ':cons, 'ུ':vow, '་':tsek, \n",
      "21. 'འ':cons, 'ཐ':cons, 'ུ':vow, 'ང':cons, '་':tsek, \n",
      "22. '།':punct, '།':punct, ' ':space, '།':punct, '།':punct, \n",
      "23. 'མ':cons, 'ཁ':cons, 'འ':cons, \n",
      "24. '།':punct, \n"
     ]
    }
   ],
   "source": [
    "for n, token in enumerate(tokens):\n",
    "    print(f'{n+1}.', end=' ')\n",
    "    for m, t in enumerate(token.char_types):\n",
    "        print(f\"'{token.content[m]}':{t}\", end=', ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
